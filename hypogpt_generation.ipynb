{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaeeun/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1, 2\"\n",
    "\n",
    "import torch\n",
    "print(torch.backends.cuda.is_built())\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(28896, 2560)\n",
       "    (wpe): Embedding(1024, 2560)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): GPT2Block(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=28896, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "\n",
    "# model_name = 'google/flan-t5-small'\n",
    "# model_name = 'google/flan-t5-base'\n",
    "# model_name = 'google/flan-t5-small'\n",
    "# model_name = 'google/flan-t5-xl'\n",
    "# model_name = 'google/flan-t5-large'\n",
    "# model_name = 'google/flan-t5-xxl'\n",
    "# model_name = \"google/pegasus-pubmed\"\n",
    "# model_name = 'microsoft/BioGPT-Large'\n",
    "\n",
    "model_name = \"stanford-crfm/BioMedLM\"\n",
    "\n",
    "\n",
    "# config.n_positions = 2048\n",
    "\n",
    "\n",
    "if model_name == \"microsoft/BioGPT-Large\":\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
    "elif model_name == \"stanford-crfm/BioMedLM\":\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "else:\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"stanford-crfm/BioMedLM\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 28895,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 28895,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 2560,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 32,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": true,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 28896\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## experimenting for BioMedLM. Delete this cell later. \n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models to run\n",
    "# bioGPT\n",
    "# bioGPT large\n",
    "# Flan-T5\n",
    "# google/pegasus-pubmed\n",
    "\n",
    "# ['base', 'FG_template', 'evidence-only']\n",
    "\n",
    "num_shots = 1\n",
    "mode = 'base'\n",
    "# instruction = None\n",
    "instruction = \"Given the evidence, answer the query provided: \"\n",
    "# instruction = \"Based on the evidence provided, what is your conclusion regarding the query? \"\n",
    "# instruction = \"Write a summary of the evidence and query in your own words: \"\n",
    "# instruction = \"What are the potential implications of the given evidences in relation to the query? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs propt template with tags to be replaced\n",
    "# could avoid the problem of overlapping with query by splitting contexts before passing it to the two functions \n",
    "import random\n",
    "# (contexts, queries, mode, num_shots, instruction=None):\n",
    "class Prompt:\n",
    "    def __init__(self, contexts, queries):\n",
    "        self.contexts = contexts\n",
    "        self.queries = queries\n",
    "        # self.mode = mode \n",
    "        # self.num_shots = num_shots\n",
    "            \n",
    "    def get_context_template(self, contexts_subset, num_examples): # or you could return all possible templates as a list \n",
    "        \n",
    "        if num_examples==0:\n",
    "            return None\n",
    "            \n",
    "        # prefix on every example, suffix only at the very end of context template\n",
    "        example_part = \"\"\n",
    "        for idx_example in range(num_examples): # change this part for random sampling / all permutation / etc\n",
    "            example = contexts_subset[idx_example]\n",
    "            example_part += \"<EX_PREFIX>\"\n",
    "            for i, ev in enumerate(example['evidences']):\n",
    "                example_part += f\"evidence {i}: {ev} \\n\"\n",
    "            \n",
    "            example_part += f\"query: {example['query']} \\n\"\n",
    "            example_part += \"<EX_OUTPUT>\" # either Output: or Instruction or both?\n",
    "            example_part += f\"{example['ground_truth']} \\n\"\n",
    "        example_part += \"<EX_SUFFIX>\"\n",
    "            \n",
    "        return example_part\n",
    "\n",
    "    def get_query_templates(self, queries_subset):\t# returns a list of all possible query templates\n",
    "        \n",
    "        templates = []\n",
    "        # dict_3 = {**dict_1,**dict_2}\n",
    "        \n",
    "        for query_idx in range(len(queries_subset)):\n",
    "            query = queries_subset[query_idx]\n",
    "\n",
    "            query_part = \"<QR_PREFIX>\\n\" # probs will just get rid of it \n",
    "            for i, ev in enumerate(query['evidences']):\n",
    "                query_part += f\"evidence {i}: {ev} \\n\"\n",
    "\n",
    "            query_part += f\"query: {query['query']} \\n\"\n",
    "            query_part += \"<QR_SUFFIX>\" # probs instruction\n",
    "            \n",
    "            templates.append(query_part)\n",
    "            \n",
    "        return templates\n",
    "\n",
    "    def transform_templates(self, context_template, query_templates, mode, instruction):\n",
    "\n",
    "        ## FG_template이면 query가 없고 evidence-only면 ... 뭐가 없지? 얘는 context를 날리는게 맞는 듯. \n",
    "        \n",
    "        assert (mode in ['base', 'FG_template', 'evidence-only']), \"invalid mode\"\n",
    "        \n",
    "        instruction = (\"Summarize given evidences and query: \" if instruction==None else instruction)\n",
    "        preface = \"I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs: \\n\"\n",
    "        \n",
    "        if mode==\"base\":\n",
    "            ex_prefix, ex_output, ex_suffix, qr_prefix, qr_suffix = \"\", instruction, \"\", \"\", instruction\n",
    "        elif mode==\"FG_template\": # query part irrelevant\n",
    "            ex_prefix, ex_output, ex_suffix = (preface + \"\\n\" + \"Input: \\n\"), \"Output: \\n\", \"\\nThe instruction was: \"\n",
    "        elif mode==\"evidence-only\": # context part irrelevant\n",
    "            context_template = None\n",
    "            qr_prefix, qr_suffix = \"\", \"\",\n",
    "            \n",
    "        # \n",
    "        context_transformed = context_template.replace('<EX_PREFIX>', ex_prefix).replace('<EX_OUTPUT>', ex_output).replace('<EX_SUFFIX>', ex_suffix) if (context_template!=None) else \"\"\n",
    "        \n",
    "        if mode=='FG_template':\n",
    "            queries_transformed = None\n",
    "        else:\n",
    "            queries_transformed = []\n",
    "            for query_template in query_templates:\n",
    "                temp = query_template.replace('<QR_PREFIX>', qr_prefix).replace('<QR_SUFFIX>', qr_suffix)\n",
    "                queries_transformed.append(temp)\n",
    "            \n",
    "        return context_transformed, queries_transformed\n",
    "        \n",
    "\n",
    "\n",
    "    def get_prompt(self, mode, num_shots, instruction=None):\n",
    "        \n",
    "\n",
    "        # randomly select from contexts num_shots contexts\n",
    "        indices = list(range(len(self.contexts)))  # create a list of indices from 0 to 9\n",
    "        context_indices = random.sample(indices, num_shots)\n",
    "        contexts = [self.contexts[i] for i in context_indices]\n",
    "        as_queries = [self.contexts[i] for i in range(len(self.contexts)) if i not in context_indices]\n",
    "        queries = self.queries + as_queries\n",
    "        \n",
    "        context_template = self.get_context_template(contexts, num_shots)\n",
    "        query_templates = self.get_query_templates(queries)\n",
    "        \n",
    "        context_transformed, queries_transformed = self.transform_templates(context_template, query_templates, mode, instruction)\n",
    "        \n",
    "        prompts = []\n",
    "        if mode != 'FG_template':\n",
    "            for query_transformed in queries_transformed:\n",
    "                prompt = context_transformed + query_transformed\n",
    "                prompts.append(prompt)\n",
    "        else:\n",
    "            prompts.append(context_transformed) # if mode is FG_template\n",
    "        \n",
    "        return prompts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for Google Sheets \n",
    "\n",
    "item1 = '''Here we report the characterization of the human Notch3 gene which we mapped to the CADASIL critical region. We have identified mutations in CADASIL patients that cause serious disruption of this gene, indicating that Notch3 could be the defective protein in CADASIL patients. All these missense mutations may result in severe disruption of the Notch3 protein, as suggested by the highly conserved nature of the aminoacid residues involved, particularly the cysteines that are key features of EGF likedomains24. These results indicate that these nucleotide substitutions are pathogenic mutations rather than rare polymorphisms.'''\n",
    "item2 = '''Linkage studies in other families enabled further refinement of this genetic interval16,17 and identification of the mutated gene as NOTCH3 (Notch homolog 3).18'''\n",
    "item3 = '''CADASIL, a hereditary vascular dementia, suggesting a role for Notch3 in vessel homeostasis (Joutel et al. 1996). CADASIL is a late-onset disorder, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of vSMC . maturation of vSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of vSMC.'''\n",
    "\n",
    "item3 = \"There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \"\n",
    "item4 = \"We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \"\n",
    "item5 = \"Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large\"\n",
    "item6 = \"Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK.\"\n",
    "item7 = \"Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-κB. A positive regulation between NOTCH and NF-κB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \"\n",
    "\n",
    "gt = \"Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGF-b1 signaling.\"\n",
    "\n",
    "set1 = {\"query\": 'What is the cause of the CADASIL?', \"evidences\": [item1, item2, item3]}\n",
    "set2 = {\"query\": 'What is the therapeutic target for CADASIL?', \"evidences\": [item3, item4, item5, item6, item7], \"ground_truth\": gt}\n",
    "\n",
    "# test_args = [(set2, set1),]\n",
    "\n",
    "contexts = [set2, ]\n",
    "queries = [set1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('examples_20230207.json') as f:\n",
    "    loaded_dic = json.load(f)\n",
    "    contexts = loaded_dic['contexts']\n",
    "    queries = loaded_dic['queries']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_setup = Prompt(contexts, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36  \\nevidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease.  \\nevidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large \\nevidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \\nevidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-κB. A positive regulation between NOTCH and NF-κB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,  \\nquery: What is the therapeutic target for CADASIL? \\nGiven the evidence, answer the query provided: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGF-b1 signaling. \\n\\nevidence 0: Here we report the characterization of the human Notch3 gene which we mapped to the CADASIL critical region. We have identified mutations in CADASIL patients that cause serious disruption of this gene, indicating that Notch3 could be the defective protein in CADASIL patients. All these missense mutations may result in severe disruption of the Notch3 protein, as suggested by the highly conserved nature of the aminoacid residues involved, particularly the cysteines that are key features of EGF likedomains24. These results indicate that these nucleotide substitutions are pathogenic mutations rather than rare polymorphisms. \\nevidence 1: Linkage studies in other families enabled further refinement of this genetic interval16,17 and identification of the mutated gene as NOTCH3 (Notch homolog 3).18 \\nevidence 2: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36  \\nquery: What is the cause of the CADASIL? \\nGiven the evidence, answer the query provided: ']\n"
     ]
    }
   ],
   "source": [
    "# assert (mode in ['base', 'FG_template', 'evidence-only']), \"invalid mode\"\n",
    "\n",
    "# num_shots = 3\n",
    "# mode = 'FG_template'\n",
    "# instruction = ''\n",
    "\n",
    "prompts = prompt_setup.get_prompt(mode=mode, num_shots=num_shots, instruction=instruction)\n",
    "\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chaeeun/anaconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/grad_mode.py\n",
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        inputs: Optional[torch.Tensor] = None,\n",
      "        max_length: Optional[int] = None,\n",
      "        min_length: Optional[int] = None,\n",
      "        do_sample: Optional[bool] = None,\n",
      "        early_stopping: Optional[bool] = None,\n",
      "        num_beams: Optional[int] = None,\n",
      "        temperature: Optional[float] = None,\n",
      "        penalty_alpha: Optional[float] = None,\n",
      "        top_k: Optional[int] = None,\n",
      "        top_p: Optional[float] = None,\n",
      "        typical_p: Optional[float] = None,\n",
      "        repetition_penalty: Optional[float] = None,\n",
      "        bad_words_ids: Optional[Iterable[int]] = None,\n",
      "        force_words_ids: Optional[Union[Iterable[int], Iterable[Iterable[int]]]] = None,\n",
      "        bos_token_id: Optional[int] = None,\n",
      "        pad_token_id: Optional[int] = None,\n",
      "        eos_token_id: Optional[int] = None,\n",
      "        length_penalty: Optional[float] = None,\n",
      "        no_repeat_ngram_size: Optional[int] = None,\n",
      "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
      "        num_return_sequences: Optional[int] = None,\n",
      "        max_time: Optional[float] = None,\n",
      "        max_new_tokens: Optional[int] = None,\n",
      "        decoder_start_token_id: Optional[int] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        num_beam_groups: Optional[int] = None,\n",
      "        diversity_penalty: Optional[float] = None,\n",
      "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
      "        logits_processor: Optional[LogitsProcessorList] = None,\n",
      "        renormalize_logits: Optional[bool] = None,\n",
      "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
      "        constraints: Optional[List[Constraint]] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        output_scores: Optional[bool] = None,\n",
      "        return_dict_in_generate: Optional[bool] = None,\n",
      "        forced_bos_token_id: Optional[int] = None,\n",
      "        forced_eos_token_id: Optional[int] = None,\n",
      "        remove_invalid_values: Optional[bool] = None,\n",
      "        synced_gpus: Optional[bool] = False,\n",
      "        exponential_decay_length_penalty: Optional[Tuple[int, float]] = None,\n",
      "        suppress_tokens: Optional[List[int]] = None,\n",
      "        begin_suppress_tokens: Optional[List[int]] = None,\n",
      "        forced_decoder_ids: Optional[List[List[int]]] = None,\n",
      "        **model_kwargs,\n",
      "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
      "        r\"\"\"\n",
      "\n",
      "        Generates sequences of token ids for models with a language modeling head. The method supports the following\n",
      "        generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
      "\n",
      "            - *greedy decoding* by calling [`~generation_utils.GenerationMixin.greedy_search`] if `num_beams=1` and\n",
      "              `do_sample=False`.\n",
      "            - *contrastive search* by calling [`~generation_utils.GenerationMixin.contrastive_search`] if\n",
      "              `penalty_alpha>0.` and `top_k>1`\n",
      "            - *multinomial sampling* by calling [`~generation_utils.GenerationMixin.sample`] if `num_beams=1` and\n",
      "              `do_sample=True`.\n",
      "            - *beam-search decoding* by calling [`~generation_utils.GenerationMixin.beam_search`] if `num_beams>1` and\n",
      "              `do_sample=False`.\n",
      "            - *beam-search multinomial sampling* by calling [`~generation_utils.GenerationMixin.beam_sample`] if\n",
      "              `num_beams>1` and `do_sample=True`.\n",
      "            - *diverse beam-search decoding* by calling [`~generation_utils.GenerationMixin.group_beam_search`], if\n",
      "              `num_beams>1` and `num_beam_groups>1`.\n",
      "            - *constrained beam-search decoding* by calling\n",
      "              [`~generation_utils.GenerationMixin.constrained_beam_search`], if `constraints!=None` or\n",
      "              `force_words_ids!=None`.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as\n",
      "        defined in the model's config (`config.json`) which in turn defaults to the\n",
      "        [`~modeling_utils.PretrainedConfig`] of the model.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "        Most of these parameters are explained in more detail in [this blog\n",
      "        post](https://huggingface.co/blog/how-to-generate).\n",
      "\n",
      "        Parameters:\n",
      "            inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "                should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "                `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "            max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
      "                The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
      "                `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in\n",
      "                the prompt.\n",
      "            max_new_tokens (`int`, *optional*):\n",
      "                The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
      "            min_length (`int`, *optional*, defaults to `model.config.min_length` or 10 if the config does not set any value):\n",
      "                The minimum length of the sequence to be generated.\n",
      "            do_sample (`bool`, *optional*, defaults to `model.config.do_sample` or `False` if the config does not set any value):\n",
      "                Whether or not to use sampling ; use greedy decoding otherwise.\n",
      "            early_stopping (`bool`, *optional*, defaults to `False`):\n",
      "                Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
      "            num_beams (`int`, *optional*, defaults to `model.config.num_beams` or 1 if the config does not set any value):\n",
      "                Number of beams for beam search. 1 means no beam search.\n",
      "            temperature (`float`, *optional*, defaults to `model.config.temperature` or 1.0 if the config does not set any value):\n",
      "                The value used to module the next token probabilities.\n",
      "            penalty_alpha (`float`, *optional*, defaults to `model.config.penalty_alpha` or None if the config does not set any value):\n",
      "                The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n",
      "            top_k (`int`, *optional*, defaults to `model.config.top_k` or 50 if the config does not set any value):\n",
      "                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "            top_p (`float`, *optional*, defaults to `model.config.top_p` or 1.0 if the config does not set any value):\n",
      "                If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to\n",
      "                `top_p` or higher are kept for generation.\n",
      "            typical_p (`float`, *optional*, defaults to `model.config.typical_p` or 1.0 if the config does not set any value):\n",
      "                The amount of probability mass from the original distribution to be considered in typical decoding. If\n",
      "                set to 1.0 it takes no effect. See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n",
      "            repetition_penalty (`float`, *optional*, defaults to `model.config.repetition_penalty` or 1.0 if the config does not set any value):\n",
      "                The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
      "                paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
      "            pad_token_id (`int`, *optional*, defaults to `model.config.pad_token_id`):\n",
      "                The id of the *padding* token.\n",
      "            bos_token_id (`int`, *optional*, defaults to `model.config.bos_token_id`):\n",
      "                The id of the *beginning-of-sequence* token.\n",
      "            eos_token_id (`int`, *optional*, defaults to `model.config.eos_token_id`):\n",
      "                The id of the *end-of-sequence* token.\n",
      "            length_penalty (`float`, *optional*, defaults to `model.config.length_penalty` or 1.0 if the config does not set any value):\n",
      "                Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent\n",
      "                to the sequence length, which in turn is used to divide the score of the sequence. Since the score is\n",
      "                the log likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences,\n",
      "                while `length_penalty` < 0.0 encourages shorter sequences.\n",
      "            no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.no_repeat_ngram_size` or 0 if the config does not set any value):\n",
      "                If set to int > 0, all ngrams of that size can only occur once.\n",
      "            encoder_no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.encoder_no_repeat_ngram_size` or 0 if the config does not set any value):\n",
      "                If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
      "                `decoder_input_ids`.\n",
      "            bad_words_ids(`List[List[int]]`, *optional*, defaults to `model.config.bad_words_ids`):\n",
      "                List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
      "                should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
      "                add_special_tokens=False).input_ids`.\n",
      "            force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
      "                List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
      "                list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
      "                this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
      "                where one can allow different forms of each word.\n",
      "            num_return_sequences(`int`, *optional*, defaults to `model.config.num_return_sequences` or 1 if the config does not set any value):\n",
      "                The number of independently computed returned sequences for each element in the batch.\n",
      "            max_time(`float`, *optional*):\n",
      "                The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
      "                finish the current pass after allocated time has been passed.\n",
      "            attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
      "                that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
      "                as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
      "            decoder_start_token_id (`int`, *optional*):\n",
      "                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
      "            use_cache (`bool`, *optional*, defaults to `True`):\n",
      "                Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      "                speed up decoding.\n",
      "            num_beam_groups (`int`, *optional*, defaults to `model.config.num_beam_groups` or 1 if the config does not set any value):\n",
      "                Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
      "                beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
      "            diversity_penalty (`float`, *optional*, defaults to `model.config.diversity_penalty` or 0.0 if the config does not set any value):\n",
      "                This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
      "                at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
      "                enabled.\n",
      "            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "                Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "            logits_processor (`LogitsProcessorList`, *optional*):\n",
      "                 Custom logits processors that complement the default logits processors built from arguments and a\n",
      "                 model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
      "                 config an error is thrown. This feature is intended for advanced users.\n",
      "            renormalize_logits (`bool`, *optional*, defaults to `False`):\n",
      "                Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
      "                custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
      "                score logits are normalized but some logit processors or warpers break the normalization.\n",
      "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "                 Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
      "                 model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "                 model's config an error is thrown. This feature is intended for advanced users.\n",
      "            constraints (`List[Constraint]`, *optional*):\n",
      "                 Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
      "                 of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
      "            output_attentions (`bool`, *optional*, defaults to `model.config.output_attentions` or `False` if the config does not set any value):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "                returned tensors for more details.\n",
      "            output_hidden_states (`bool`, *optional*, defaults to `model.config.output_hidden_states` or `False` if the config does not set any value):\n",
      "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      "                for more details.\n",
      "            output_scores (`bool`, *optional*, defaults to `model.config.output_scores` or `False` if the config does not set any value):\n",
      "                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      "            return_dict_in_generate (`bool`, *optional*, defaults to `model.config.return_dict_in_generate` or `False` if the config does not set any value):\n",
      "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "            forced_bos_token_id (`int`, *optional*, defaults to `model.config.forced_bos_token_id`):\n",
      "                The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
      "                for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
      "                the target language token.\n",
      "            forced_eos_token_id (`int`, *optional*, defaults to `model.config.forced_eos_token_id`):\n",
      "                The id of the token to force as the last generated token when `max_length` is reached.\n",
      "            remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n",
      "                Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
      "                crash. Note that using `remove_invalid_values` can slow down generation.\n",
      "            synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      "                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      "            exponential_decay_length_penalty (`tuple(int, float)`, *optional*, defaults to `model.config.exponential_decay_length_penalty`):\n",
      "                This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
      "                generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
      "                where penalty starts and `decay_factor` represents the factor of exponential decay\n",
      "            suppress_tokens  (`List[int]`, *optional*, defaults to `model.config.suppress_tokens`):\n",
      "                A list of tokens that will be supressed at generation. The `SupressTokens` logit processor will set\n",
      "                their log probs to `-inf` so that they are not sampled.\n",
      "            begin_suppress_tokens  (`List[int]`, *optional*, defaults to `model.config.begin_suppress_tokens`):\n",
      "                A list of tokens that will be supressed at the begining of the generation. The `SupressBeginTokens`\n",
      "                logit processor will set their log probs to `-inf` so that they are not sampled.\n",
      "            forced_decoder_ids (`List[List[int]]`, *optional*, defaults to `model.config.forced_decoder_ids`):\n",
      "                A list of pairs of integers which indicates a mapping from generation indices to token indices that\n",
      "                will be forced before sampling. For example, `[[1, 123]]` means the second generated token will always\n",
      "                be a token of index 123.\n",
      "            model_kwargs:\n",
      "                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
      "                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
      "                should be prefixed with *decoder_*.\n",
      "\n",
      "        Return:\n",
      "            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      "\n",
      "                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
      "                    - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
      "                    - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
      "                    - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
      "\n",
      "                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
      "                    - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
      "                    - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
      "                    - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
      "\n",
      "        Examples:\n",
      "\n",
      "        Greedy Decoding:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "\n",
      "        >>> prompt = \"Today I believe we can finally\"\n",
      "        >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
      "\n",
      "        >>> # generate up to 30 tokens\n",
      "        >>> outputs = model.generate(input_ids, do_sample=False, max_length=30)\n",
      "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      "        ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n']\n",
      "        ```\n",
      "\n",
      "        Multinomial Sampling:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "        >>> import torch\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "\n",
      "        >>> prompt = \"Today I believe we can finally\"\n",
      "        >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
      "\n",
      "        >>> # sample up to 30 tokens\n",
      "        >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
      "        >>> outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
      "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      "        ['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']\n",
      "        ```\n",
      "\n",
      "        Beam-search decoding:\n",
      "\n",
      "        ```python\n",
      "        >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
      "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
      "\n",
      "        >>> sentence = \"Paris is one of the densest populated areas in Europe.\"\n",
      "        >>> input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
      "\n",
      "        >>> outputs = model.generate(input_ids, num_beams=5)\n",
      "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      "        ['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\n",
      "        ```\"\"\"\n",
      "        # 0. Validate the `.generate()` call\n",
      "        self._validate_model_class()\n",
      "        self._validate_model_kwargs(model_kwargs.copy())\n",
      "\n",
      "        # 1. Set generation parameters if not already defined\n",
      "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
      "        num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
      "        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
      "        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
      "        num_beam_groups = num_beam_groups if num_beam_groups is not None else self.config.num_beam_groups\n",
      "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
      "        num_return_sequences = (\n",
      "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
      "        )\n",
      "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
      "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
      "\n",
      "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
      "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
      "\n",
      "        if eos_token_id is None and hasattr(self.config, \"decoder\"):\n",
      "            eos_token_id = self.config.decoder.eos_token_id\n",
      "\n",
      "        if pad_token_id is None and eos_token_id is not None:\n",
      "            if model_kwargs.get(\"attention_mask\", None) is None:\n",
      "                logger.warning(\n",
      "                    \"The attention mask and the pad token id were not set. As a consequence, you may observe \"\n",
      "                    \"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
      "                )\n",
      "            logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
      "            pad_token_id = eos_token_id\n",
      "\n",
      "        output_scores = output_scores if output_scores is not None else self.config.output_scores\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        return_dict_in_generate = (\n",
      "            return_dict_in_generate if return_dict_in_generate is not None else self.config.return_dict_in_generate\n",
      "        )\n",
      "\n",
      "        # 2. Define model inputs\n",
      "        # inputs_tensor has to be defined\n",
      "        # model_input_name is defined if model-specific keyword input is passed\n",
      "        # otherwise model_input_name is None\n",
      "        # all model-specific keyword inputs are removed from `model_kwargs`\n",
      "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n",
      "        batch_size = inputs_tensor.shape[0]\n",
      "\n",
      "        # 3. Define other model kwargs\n",
      "        model_kwargs[\"output_attentions\"] = output_attentions\n",
      "        model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
      "        model_kwargs[\"use_cache\"] = use_cache\n",
      "\n",
      "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
      "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
      "\n",
      "        if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
      "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
      "                inputs_tensor, pad_token_id, eos_token_id\n",
      "            )\n",
      "\n",
      "        # decoder-only models should use left-padding for generation\n",
      "        if not self.config.is_encoder_decoder:\n",
      "            if pad_token_id is not None and torch.sum(inputs_tensor[:, -1] == pad_token_id) > 0:\n",
      "                logger.warning(\n",
      "                    \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n",
      "                    \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
      "                )\n",
      "\n",
      "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
      "            # if model is encoder decoder encoder_outputs are created\n",
      "            # and added to `model_kwargs`\n",
      "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "                inputs_tensor, model_kwargs, model_input_name\n",
      "            )\n",
      "\n",
      "        # 4. Prepare `input_ids` which will be used for auto-regressive generation\n",
      "        if self.config.is_encoder_decoder:\n",
      "            input_ids = self._prepare_decoder_input_ids_for_generation(\n",
      "                batch_size,\n",
      "                decoder_start_token_id=decoder_start_token_id,\n",
      "                bos_token_id=bos_token_id,\n",
      "                model_kwargs=model_kwargs,\n",
      "                device=inputs_tensor.device,\n",
      "            )\n",
      "        else:\n",
      "            # if decoder-only then inputs_tensor has to be `input_ids`\n",
      "            input_ids = inputs_tensor\n",
      "\n",
      "        # 5. Prepare `max_length` depending on other stopping criteria.\n",
      "        input_ids_seq_length = input_ids.shape[-1]\n",
      "        if max_length is None and max_new_tokens is None:\n",
      "            warnings.warn(\n",
      "                \"Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to \"\n",
      "                f\"{self.config.max_length} (`self.config.max_length`). Controlling `max_length` via the config is \"\n",
      "                \"deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend \"\n",
      "                \"using `max_new_tokens` to control the maximum length of the generation.\",\n",
      "                UserWarning,\n",
      "            )\n",
      "        elif max_length is None and max_new_tokens is not None:\n",
      "            max_length = max_new_tokens + input_ids_seq_length\n",
      "        elif max_length is not None and max_new_tokens is not None:\n",
      "            raise ValueError(\n",
      "                \"Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a\"\n",
      "                \" limit to the generated output length. Remove one of those arguments. Please refer to the\"\n",
      "                \" documentation for more information. \"\n",
      "                \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n",
      "            )\n",
      "        # default to config if still None\n",
      "        max_length = max_length if max_length is not None else self.config.max_length\n",
      "        min_length = min_length if min_length is not None else self.config.min_length\n",
      "\n",
      "        if min_length is not None and min_length > max_length:\n",
      "            raise ValueError(\n",
      "                f\"Unfeasible length constraints: the minimum length ({min_length}) is larger than the maximum \"\n",
      "                f\"length ({max_length})\"\n",
      "            )\n",
      "        if input_ids_seq_length >= max_length:\n",
      "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
      "            logger.warning(\n",
      "                f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
      "                f\" {max_length}. This can lead to unexpected behavior. You should consider increasing \"\n",
      "                \"`max_new_tokens`.\"\n",
      "            )\n",
      "\n",
      "        # 6. determine generation mode\n",
      "        is_constraint_gen_mode = constraints is not None or force_words_ids is not None\n",
      "\n",
      "        is_contrastive_search_gen_mode = (\n",
      "            top_k is not None and top_k > 1 and do_sample is False and penalty_alpha is not None and penalty_alpha > 0\n",
      "        )\n",
      "\n",
      "        is_greedy_gen_mode = (\n",
      "            (num_beams == 1)\n",
      "            and (num_beam_groups == 1)\n",
      "            and do_sample is False\n",
      "            and not is_constraint_gen_mode\n",
      "            and not is_contrastive_search_gen_mode\n",
      "        )\n",
      "        is_sample_gen_mode = (\n",
      "            (num_beams == 1)\n",
      "            and (num_beam_groups == 1)\n",
      "            and do_sample is True\n",
      "            and not is_constraint_gen_mode\n",
      "            and not is_contrastive_search_gen_mode\n",
      "        )\n",
      "        is_beam_gen_mode = (\n",
      "            (num_beams > 1)\n",
      "            and (num_beam_groups == 1)\n",
      "            and do_sample is False\n",
      "            and not is_constraint_gen_mode\n",
      "            and not is_contrastive_search_gen_mode\n",
      "        )\n",
      "        is_beam_sample_gen_mode = (\n",
      "            (num_beams > 1)\n",
      "            and (num_beam_groups == 1)\n",
      "            and do_sample is True\n",
      "            and not is_constraint_gen_mode\n",
      "            and not is_contrastive_search_gen_mode\n",
      "        )\n",
      "        is_group_beam_gen_mode = (\n",
      "            (num_beams > 1)\n",
      "            and (num_beam_groups > 1)\n",
      "            and not is_constraint_gen_mode\n",
      "            and not is_contrastive_search_gen_mode\n",
      "        )\n",
      "\n",
      "        if num_beam_groups > num_beams:\n",
      "            raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
      "        if is_group_beam_gen_mode and do_sample is True:\n",
      "            raise ValueError(\n",
      "                \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
      "            )\n",
      "\n",
      "        if self.device.type != input_ids.device.type:\n",
      "            warnings.warn(\n",
      "                \"You are calling .generate() with the `input_ids` being on a device type different\"\n",
      "                f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n",
      "                f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\"\n",
      "                \" Please make sure that you have put `input_ids` to the\"\n",
      "                f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\"\n",
      "                \" running `.generate()`.\",\n",
      "                UserWarning,\n",
      "            )\n",
      "\n",
      "        # 7. prepare distribution pre_processing samplers\n",
      "        logits_processor = self._get_logits_processor(\n",
      "            repetition_penalty=repetition_penalty,\n",
      "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
      "            encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
      "            input_ids_seq_length=input_ids_seq_length,\n",
      "            encoder_input_ids=inputs_tensor,\n",
      "            bad_words_ids=bad_words_ids,\n",
      "            min_length=min_length,\n",
      "            max_length=max_length,\n",
      "            eos_token_id=eos_token_id,\n",
      "            forced_bos_token_id=forced_bos_token_id,\n",
      "            forced_eos_token_id=forced_eos_token_id,\n",
      "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
      "            num_beams=num_beams,\n",
      "            num_beam_groups=num_beam_groups,\n",
      "            diversity_penalty=diversity_penalty,\n",
      "            remove_invalid_values=remove_invalid_values,\n",
      "            exponential_decay_length_penalty=exponential_decay_length_penalty,\n",
      "            logits_processor=logits_processor,\n",
      "            renormalize_logits=renormalize_logits,\n",
      "            suppress_tokens=suppress_tokens,\n",
      "            begin_suppress_tokens=begin_suppress_tokens,\n",
      "            forced_decoder_ids=forced_decoder_ids,\n",
      "        )\n",
      "\n",
      "        # 8. prepare stopping criteria\n",
      "        stopping_criteria = self._get_stopping_criteria(\n",
      "            max_length=max_length, max_time=max_time, stopping_criteria=stopping_criteria\n",
      "        )\n",
      "        # 9. go into different generation modes\n",
      "        if is_greedy_gen_mode:\n",
      "            if num_return_sequences > 1:\n",
      "                raise ValueError(\n",
      "                    f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
      "                )\n",
      "\n",
      "            # 10. run greedy search\n",
      "            return self.greedy_search(\n",
      "                input_ids,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                output_scores=output_scores,\n",
      "                return_dict_in_generate=return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif is_contrastive_search_gen_mode:\n",
      "\n",
      "            if num_return_sequences > 1:\n",
      "                raise ValueError(\n",
      "                    f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing contrastive search.\"\n",
      "                )\n",
      "\n",
      "            return self.contrastive_search(\n",
      "                input_ids,\n",
      "                top_k=top_k,\n",
      "                penalty_alpha=penalty_alpha,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                output_scores=output_scores,\n",
      "                return_dict_in_generate=return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif is_sample_gen_mode:\n",
      "            # 10. prepare logits warper\n",
      "            logits_warper = self._get_logits_warper(\n",
      "                top_k=top_k,\n",
      "                top_p=top_p,\n",
      "                typical_p=typical_p,\n",
      "                temperature=temperature,\n",
      "                num_beams=num_beams,\n",
      "                renormalize_logits=renormalize_logits,\n",
      "            )\n",
      "\n",
      "            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids,\n",
      "                expand_size=num_return_sequences,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "            # 12. run sample\n",
      "            return self.sample(\n",
      "                input_ids,\n",
      "                logits_processor=logits_processor,\n",
      "                logits_warper=logits_warper,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                output_scores=output_scores,\n",
      "                return_dict_in_generate=return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif is_beam_gen_mode:\n",
      "            if num_return_sequences > num_beams:\n",
      "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
      "\n",
      "            if stopping_criteria.max_length is None:\n",
      "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
      "\n",
      "            # 10. prepare beam search scorer\n",
      "            beam_scorer = BeamSearchScorer(\n",
      "                batch_size=batch_size,\n",
      "                num_beams=num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=length_penalty,\n",
      "                do_early_stopping=early_stopping,\n",
      "                num_beam_hyps_to_keep=num_return_sequences,\n",
      "            )\n",
      "            # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
      "            )\n",
      "            # 12. run beam search\n",
      "            return self.beam_search(\n",
      "                input_ids,\n",
      "                beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                output_scores=output_scores,\n",
      "                return_dict_in_generate=return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif is_beam_sample_gen_mode:\n",
      "            # 10. prepare logits warper\n",
      "            logits_warper = self._get_logits_warper(\n",
      "                top_k=top_k,\n",
      "                top_p=top_p,\n",
      "                typical_p=typical_p,\n",
      "                temperature=temperature,\n",
      "                num_beams=num_beams,\n",
      "                renormalize_logits=renormalize_logits,\n",
      "            )\n",
      "\n",
      "            if stopping_criteria.max_length is None:\n",
      "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
      "            # 11. prepare beam search scorer\n",
      "            beam_scorer = BeamSearchScorer(\n",
      "                batch_size=batch_size * num_return_sequences,\n",
      "                num_beams=num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=length_penalty,\n",
      "                do_early_stopping=early_stopping,\n",
      "            )\n",
      "\n",
      "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids,\n",
      "                expand_size=num_beams * num_return_sequences,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "            # 13. run beam sample\n",
      "            return self.beam_sample(\n",
      "                input_ids,\n",
      "                beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                logits_warper=logits_warper,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                output_scores=output_scores,\n",
      "                return_dict_in_generate=return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif is_group_beam_gen_mode:\n",
      "            if num_return_sequences > num_beams:\n",
      "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
      "\n",
      "            if num_beams % num_beam_groups != 0:\n",
      "                raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
      "\n",
      "            if stopping_criteria.max_length is None:\n",
      "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
      "\n",
      "            if typical_p is not None:\n",
      "                raise ValueError(\"Decoder argument `typical_p` is not supported with beam groups.\")\n",
      "\n",
      "            # 10. prepare beam search scorer\n",
      "            beam_scorer = BeamSearchScorer(\n",
      "                batch_size=batch_size,\n",
      "                num_beams=num_beams,\n",
      "                max_length=stopping_criteria.max_length,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=length_penalty,\n",
      "                do_early_stopping=early_stopping,\n",
      "                num_beam_hyps_to_keep=num_return_sequences,\n",
      "                num_beam_groups=num_beam_groups,\n",
      "            )\n",
      "            # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
      "            )\n",
      "            # 12. run beam search\n",
      "            return self.group_beam_search(\n",
      "                input_ids,\n",
      "                beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                output_scores=output_scores,\n",
      "                return_dict_in_generate=return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif is_constraint_gen_mode:\n",
      "            if num_return_sequences > num_beams:\n",
      "                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
      "\n",
      "            if stopping_criteria.max_length is None:\n",
      "                raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
      "\n",
      "            if num_beams <= 1:\n",
      "                raise ValueError(\"`num_beams` needs to be greater than 1 for constrained generation.\")\n",
      "\n",
      "            if do_sample:\n",
      "                raise ValueError(\"`do_sample` needs to be false for constrained generation.\")\n",
      "\n",
      "            if num_beam_groups is not None and num_beam_groups > 1:\n",
      "                raise ValueError(\"`num_beam_groups` not supported yet for constrained generation.\")\n",
      "\n",
      "            final_constraints = []\n",
      "            if constraints is not None:\n",
      "                final_constraints = constraints\n",
      "\n",
      "            if force_words_ids is not None:\n",
      "\n",
      "                def typeerror():\n",
      "                    raise ValueError(\n",
      "                        \"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]`\"\n",
      "                        f\"of positive integers, but is {force_words_ids}.\"\n",
      "                    )\n",
      "\n",
      "                if not isinstance(force_words_ids, list) or len(force_words_ids) == 0:\n",
      "                    typeerror()\n",
      "\n",
      "                for word_ids in force_words_ids:\n",
      "                    if isinstance(word_ids[0], list):\n",
      "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
      "                            typeerror()\n",
      "                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n",
      "                            typeerror()\n",
      "                        if any(\n",
      "                            any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids)\n",
      "                            for token_ids in word_ids\n",
      "                        ):\n",
      "                            typeerror()\n",
      "\n",
      "                        constraint = DisjunctiveConstraint(word_ids)\n",
      "                    else:\n",
      "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
      "                            typeerror()\n",
      "                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n",
      "                            typeerror()\n",
      "\n",
      "                        constraint = PhrasalConstraint(word_ids)\n",
      "                    final_constraints.append(constraint)\n",
      "\n",
      "            # 10. prepare beam search scorer\n",
      "            constrained_beam_scorer = ConstrainedBeamSearchScorer(\n",
      "                constraints=final_constraints,\n",
      "                batch_size=batch_size,\n",
      "                num_beams=num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=length_penalty,\n",
      "                do_early_stopping=early_stopping,\n",
      "                num_beam_hyps_to_keep=num_return_sequences,\n",
      "            )\n",
      "            # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids, expand_size=num_beams, is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs\n",
      "            )\n",
      "            # 12. run beam search\n",
      "            return self.constrained_beam_search(\n",
      "                input_ids,\n",
      "                constrained_beam_scorer=constrained_beam_scorer,\n",
      "                logits_processor=logits_processor,\n",
      "                stopping_criteria=stopping_criteria,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                output_scores=output_scores,\n",
      "                return_dict_in_generate=return_dict_in_generate,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## experiment. delete this cell later. \n",
    "\n",
    "import inspect \n",
    "\n",
    "lines = inspect.getsource(model.generate)\n",
    "\n",
    "print(inspect.getsourcefile(model.generate))\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in progress. simplifying generation configuration process\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_length\": 1024,\n",
    "    \"min_length\": 1024,\n",
    "    'do_sample': True, \n",
    "    'top_k': 50,\n",
    "    'num_beams': 5,   \n",
    "}\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"max_length\": 1024,\n",
    "    \"return_tensors\": \"pt\", # biomed by default only specifies this\n",
    "    \"truncation\": True,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:28895 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 11.91 GiB total capacity; 10.98 GiB already allocated; 152.94 MiB free; 11.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[39m# outputs = model.generate(**inputs, max_length=2048, min_length=100, num_beams=5)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m     25\u001b[0m generated \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(generated)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation_utils.py:1621\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1613\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1614\u001b[0m         input_ids,\n\u001b[1;32m   1615\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_beams \u001b[39m*\u001b[39m num_return_sequences,\n\u001b[1;32m   1616\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1617\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1618\u001b[0m     )\n\u001b[1;32m   1620\u001b[0m     \u001b[39m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_sample(\n\u001b[1;32m   1622\u001b[0m         input_ids,\n\u001b[1;32m   1623\u001b[0m         beam_scorer,\n\u001b[1;32m   1624\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1625\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1626\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1627\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1628\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1629\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1630\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1631\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1632\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1635\u001b[0m \u001b[39melif\u001b[39;00m is_group_beam_gen_mode:\n\u001b[1;32m   1636\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation_utils.py:3059\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3055\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3057\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3060\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3061\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3062\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3063\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3064\u001b[0m )\n\u001b[1;32m   3066\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3067\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1046\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1047\u001b[0m     input_ids,\n\u001b[1;32m   1048\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1049\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1050\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1051\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1052\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1053\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1054\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1055\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1056\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1057\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1058\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1059\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1060\u001b[0m )\n\u001b[1;32m   1061\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1063\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:889\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    879\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    880\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    881\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    890\u001b[0m         hidden_states,\n\u001b[1;32m    891\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    892\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    893\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    894\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    895\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    896\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    897\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    900\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    425\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 426\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    427\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:354\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[39m.\u001b[39mFloatTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m    353\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 354\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(hidden_states)\n\u001b[1;32m    355\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/activations.py:35\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(math\u001b[39m.\u001b[39msqrt(\u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39mpi) \u001b[39m*\u001b[39m (\u001b[39minput\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mpow(\u001b[39minput\u001b[39;49m, \u001b[39m3.0\u001b[39;49m))))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 11.91 GiB total capacity; 10.98 GiB already allocated; 152.94 MiB free; 11.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "num_to_word = {}\n",
    "num_to_word[0]='zero-shot'\n",
    "num_to_word[1]='one-shot'\n",
    "num_to_word[2]='two-shot'\n",
    "num_to_word[3]='few-shot'\n",
    "\n",
    "output_file_name = f\"{model_name.replace('/', '-')}_{mode}-mode_{num_to_word[num_shots]}\"\n",
    "output_dic = {\n",
    "    'output' : [],\n",
    "    'config' : {'mode': mode, 'num_shots': num_shots}\n",
    "}\n",
    "\n",
    "\n",
    "# cwd = os.path.dirname(os.path.abspath(__file__))\n",
    "cwd = '/home/chaeeun/workspace/biochatgpt_generation'\n",
    "file_name_json = os.path.join(cwd, f\"{output_file_name}.json\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    \n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = tokenizer(prompt, **tokenizer_kwargs)\n",
    "    inputs.to(device)\n",
    "    # outputs = model.generate(**inputs, max_length=2048, min_length=100, num_beams=5)\n",
    "    outputs = model.generate(**inputs, **generate_kwargs)\n",
    "    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(generated)\n",
    "    # generated = *generated\n",
    "    \n",
    "    output_dic['output'].append({'prompt': prompt, 'generated': generated})\n",
    "\n",
    "with open(file_name_json, \"w\") as outfile: \n",
    "    json.dump(output_dic, outfile)\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis. Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship. \n",
      "evidence 1: Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship. Neuropsychological consultation helped initiate removal of the MS diagnosis and treatment. Better understanding is needed among clinicians that MS and CADASIL are not known to co-exist, that no association has been found between MS and the NOTCH3 mutations that cause CADASIL, and that neuroimaging and clinical features can help distinguish between the two conditions in addition to genetic testing. \n",
      "evidence 2: Antithrombotic drugs were used in three patients, in one for an unrelated coexisting prothrombotic condition. CADASIL does not seem to be associated with an unfavorable outcome of pregnancy either for women and fetuses. Patients and treating physicians should be reassured that pregnancy can be safely initiated in CADASIL, as there is no evidence to support a specific preventive antithrombotic treatment during pregnancy in CADASIL. \n",
      "query: can nilvadipine help alleviate cadasil symptoms? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Using both wild type mice and transgenic mice carrying the human mutant Notch3 gene (CADASIL mice), we have recently characterized the pathological features of CADASIL and determined the therapeutic efficacy of two hematopoietic growth factors, stem cell factor (SCF) and granulocyte colony-stimulating factor (G-CSF) in CADASIL. Our findings have revealed novel pathological changes in the endothelium of cerebral capillaries and in the neural stem cells (NSCs). We have also observed the impairment of cognitive function in CADASIL mice. \n",
      "evidence 1: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available. \n",
      "evidence 2: Today, CADASIL is known to every neurologist, but the disease has not yet revealed all its secrets. A lot of effort is still needed to understand the intimate mechanisms of the disease and the most efficient targets or approaches for the development of efficient therapeutics. The history of CADASIL will be further enriched by multiple ongoing research projects worldwide, at clinical and preclinical level, and will continue to enlighten research in the field of cerebral small vessel disorders. \n",
      "query: what are the potential therapeutic targets for cadasil? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Syk inhibition was found to stabilize microtubules and potentiate paclitaxel activity in cellular models of taxane-resistant ovarian cancers. We further studied the effects of Syk inhibition on paclitaxel activity in Syk(+) ovarian cancer cell models and in variants selected for taxane resistance. Syk inhibition was accomplished using RNAi and by exposure to the small molecule competitive inhibitor R406, the active metabolite of fostamatinib. \n",
      "evidence 1: Therefore, inhibition of the inflammatory macrophages would be a promising approach to attenuate NASH. In this study, we studied the implication of SYK pathway in NASH, and investigated PLGA nanoparticles-based delivery of SYK pathway inhibitor as an effective and promising therapeutic approach for the treatment of NASH. We found positive correlation between SYK expression with the pathogenesis of NASH and alcoholic hepatitis in patients. \n",
      "evidence 2: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available. \n",
      "query: would SYK inhibition be potential treatment for cadasil? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMCs) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. Inhibition of the inflammatory macrophages would be a promising approach to attenuate NASH. In this study, we studied the implication of SYK pathway in NASH, and investigated PLGA nanoparticles-based delivery of SYK pathway inhibitor as an effective and promising therapeutic approach for the treatment of NASH. We found positive correlation between SYK expression with the pathogenesis of NASH and alcoholic hepatitis in patients.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Here we report the characterization of the human Notch3 gene which we mapped to the CADASIL critical region. We have identified mutations in CADASIL patients that cause serious disruption of this gene, indicating that Notch3 could be the defective protein in CADASIL patients.All these missense mutations may result in severe disruption of the Notch3 protein, as suggested by the highly conserved nature of the aminoacid residues involved, particularly the cysteines that are key features of EGF-like domains. These results indicate that these nucleotide substitutions are pathogenic mutations rather than rare polymorphisms. \n",
      "evidence 1: Linkage studies in other families enabled further refinement of this genetic interval16,17 and identification of the mutated gene as NOTCH3 (Notch homolog 3).18 \n",
      "evidence 2: CADASIL, a hereditary vascular dementia, suggesting a role for Notch3 in vessel homeostasis (Joutel et al. 1996). CADASIL is a late-onset disorder, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of VSMC. ...maturation of VSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of VSMC. \n",
      "query: What is the cause of the CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['CADASIL is a hereditary vascular dementia, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of VSMC....maturation of VSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of VSMC.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31). \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Question: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Role of NOTCH3 Mutations in the Cerebral Small Vessel Disease Cerebral Autosomal Dominant Arteriopathy ... (Stroke 2018). There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with... \n",
      "evidence 1: Differences in proliferation rate between CADASIL and control vascular smooth muscle cells are ... (J Cell Mol Med 2018). We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. ...showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic... \n",
      "evidence 2: The Role of Vascular Smooth Muscle Cells in Arterial Remodeling: Focus on Calcification ... (Int J Mol Sci 2019). Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused... \n",
      "evidence 3: Spleen Tyrosine Kinase (SYK) in the Progression of Peritoneal Fibrosis Through Activation of the ... (Med Sci Monit 2019). Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase... \n",
      "evidence 4: NOTCH3 signaling is essential for NF-kB activation in TLRactivated macrophages (Sci Rep 2020). Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage... \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of the TGFb1 signaling pathway.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Role of NOTCH3 Mutations in the Cerebral Small Vessel Disease Cerebral Autosomal Dominant Arteriopathy ... (Stroke 2018). There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with... \n",
      "evidence 1: NOTCH3 signaling is essential for NF-kB activation in TLRactivated macrophages (Sci Rep 2020). Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage... \n",
      "evidence 2: Niclosamide repurposed for the treatment of inflammatory airway disease (JCI Insight 2019). Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal antiinflammatory compound that inhibits goblet... \n",
      "evidence 3: The Ca2+-gated channel TMEM16A amplifies capillary pericyte contraction and reduces ... (J Clin Invest. 2022). The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of the TGFb1 signaling pathway.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{output_file_name}.txt', \"w\") as results: # mujeen_\n",
    "    results.write(f'<< Model: {model_name} >>\\n\\n')\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print('< PROMPT >\\n')\n",
    "        print(prompt)\n",
    "        results.write('< PROMPT >\\n\\n')\n",
    "        results.write(prompt)\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        inputs.to(device)\n",
    "        outputs = model.generate(**inputs, max_length=1024, min_length=100, num_beams=5)\n",
    "        generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # output_dic['output'].append({'prompt': prompt, 'generated': *generated})\n",
    "\n",
    "        print('\\n\\n< GENERATED >\\n')\n",
    "        print(generated)\n",
    "        print('\\n###############################################################################################################################################################\\n')\n",
    "        results.write('\\n\\n< GENERATED >\\n\\n')\n",
    "        results.write(*generated)\n",
    "        results.write('\\n\\n###############################################################################################################################################################\\n')\n",
    "        \n",
    "results.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce5cb1d3583c4bc4c26ba2d9d1424221025f98623b08f3ff314497f7b1d6b7f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
