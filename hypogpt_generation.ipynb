{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1, 2\"\n",
    "\n",
    "import torch\n",
    "print(torch.backends.cuda.is_built())\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models to run\n",
    "# bioGPT\n",
    "# bioGPT large\n",
    "# Flan-T5\n",
    "# google/pegasus-pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "\n",
    "model_name = 'google/flan-t5-small'\n",
    "# model_name = 'google/flan-t5-base'\n",
    "# model_name = 'google/flan-t5-small'\n",
    "# model_name = 'google/flan-t5-xl'\n",
    "# model_name = 'google/flan-t5-large'\n",
    "# model_name = 'google/flan-t5-xxl'\n",
    "# model_name = \"google/pegasus-pubmed\"\n",
    "# model_name = 'microsoft/BioGPT-Large'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.n_positions = 2048\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if model_name == \"microsoft/BioGPT-Large\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT-Large\")\n",
    "else:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs propt template with tags to be replaced\n",
    "# could avoid the problem of overlapping with query by splitting contexts before passing it to the two functions \n",
    "import random\n",
    "# (contexts, queries, mode, num_shots, instruction=None):\n",
    "class Prompt:\n",
    "    def __init__(self, contexts, queries):\n",
    "        self.contexts = contexts\n",
    "        self.queries = queries\n",
    "        # self.mode = mode \n",
    "        # self.num_shots = num_shots\n",
    "            \n",
    "    def get_context_template(self, contexts_subset, num_examples): # or you could return all possible templates as a list \n",
    "        \n",
    "        if num_examples==0:\n",
    "            return None\n",
    "            \n",
    "        \n",
    "        example_part = \"<EX_PREFIX>\"\n",
    "        for idx_example in range(num_examples): # change this part for random sampling / all permutation / etc\n",
    "            example = contexts_subset[idx_example]\n",
    "            for i, ev in enumerate(example['evidences']):\n",
    "                example_part += f\"evidence {i}: {ev} \\n\"\n",
    "            \n",
    "            example_part += f\"query: {example['query']} \\n\"\n",
    "            example_part += \"<EX_OUTPUT>\" # either Output: or Instruction or both?\n",
    "            example_part += f\"{example['ground_truth']} \\n\"\n",
    "            example_part += \"<EX_SUFFIX>\"\n",
    "            \n",
    "        return example_part\n",
    "\n",
    "    def get_query_templates(self, queries_subset):\t# returns a list of all possible query templates\n",
    "        \n",
    "        templates = []\n",
    "        # dict_3 = {**dict_1,**dict_2}\n",
    "        \n",
    "        for query_idx in range(len(queries_subset)):\n",
    "            query = queries_subset[query_idx]\n",
    "\n",
    "            query_part = \"<QR_PREFIX>\\n\" # probs will just get rid of it \n",
    "            for i, ev in enumerate(query['evidences']):\n",
    "                query_part += f\"evidence {i}: {ev} \\n\"\n",
    "\n",
    "            query_part += f\"query: {query['query']} \\n\"\n",
    "            query_part += \"<QR_SUFFIX>\" # probs instruction\n",
    "            \n",
    "            templates.append(query_part)\n",
    "            \n",
    "        return templates\n",
    "\n",
    "    def transform_templates(self, context_template, query_templates, mode, instruction):\n",
    "        \n",
    "        assert (mode in ['base', 'FG_template', 'evidence-only']), \"invalid mode\"\n",
    "        \n",
    "        instruction = (\"summarize given evidences and query:\" if instruction==None else instruction)\n",
    "        preface = \"I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs: \\n\"\n",
    "        \n",
    "        if mode==\"base\":\n",
    "            ex_prefix, ex_output, ex_suffix, qr_prefix, qr_suffix = \"\", instruction, \"\", \"\", instruction\n",
    "        elif mode==\"FG_template\": # query part irrelevant\n",
    "            ex_prefix, ex_output, ex_suffix = (preface + \"\\n\" + \"Input: \\n\"), \"Output: \\n\", preface\n",
    "        elif mode==\"evidence-only\": # context part irrelevant\n",
    "            qr_prefix, qr_suffix = \"\", \"\"\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "        context_transformed = context_template.replace('<EX_PREFIX>', ex_prefix).replace('<EX_OUTPUT>', ex_output).replace('<EX_SUFFIX>', ex_suffix) if (context_template!=None) else \"\"\n",
    "        \n",
    "        if mode=='FG_template':\n",
    "            queries_transformed = None\n",
    "        else:\n",
    "            queries_transformed = []\n",
    "            for query_template in query_templates:\n",
    "                temp = query_template.replace('<QR_PREFIX>', qr_prefix).replace('<QR_SUFFIX>', qr_suffix)\n",
    "                queries_transformed.append(temp)\n",
    "            \n",
    "        return context_transformed, queries_transformed\n",
    "        \n",
    "\n",
    "\n",
    "    def get_prompt(self, mode, num_shots, instruction=None):\n",
    "        \n",
    "\n",
    "        # randomly select from contexts num_shots contexts\n",
    "        indices = list(range(len(self.contexts)))  # create a list of indices from 0 to 9\n",
    "        context_indices = random.sample(indices, num_shots)\n",
    "        contexts = [self.contexts[i] for i in context_indices]\n",
    "        as_queries = [self.contexts[i] for i in range(len(self.contexts)) if i not in context_indices]\n",
    "        queries = self.queries + as_queries\n",
    "        \n",
    "        context_template = self.get_context_template(contexts, num_shots)\n",
    "        query_templates = self.get_query_templates(queries)\n",
    "        \n",
    "        context_transformed, queries_transformed = self.transform_templates(context_template, query_templates, mode, instruction)\n",
    "        \n",
    "        for query_transformed in queries_transformed:\n",
    "            prompt = context_transformed + query_transformed\n",
    "        \n",
    "        return prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for Google Sheets \n",
    "\n",
    "item1 = '''Here we report the characterization of the human Notch3 gene which we mapped to the CADASIL critical region. We have identified mutations in CADASIL patients that cause serious disruption of this gene, indicating that Notch3 could be the defective protein in CADASIL patients. All these missense mutations may result in severe disruption of the Notch3 protein, as suggested by the highly conserved nature of the aminoacid residues involved, particularly the cysteines that are key features of EGF likedomains24. These results indicate that these nucleotide substitutions are pathogenic mutations rather than rare polymorphisms.'''\n",
    "item2 = '''Linkage studies in other families enabled further refinement of this genetic interval16,17 and identification of the mutated gene as NOTCH3 (Notch homolog 3).18'''\n",
    "item3 = '''CADASIL, a hereditary vascular dementia, suggesting a role for Notch3 in vessel homeostasis (Joutel et al. 1996). CADASIL is a late-onset disorder, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of vSMC . maturation of vSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of vSMC.'''\n",
    "\n",
    "item3 = \"There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \"\n",
    "item4 = \"We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \"\n",
    "item5 = \"Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large\"\n",
    "item6 = \"Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK.\"\n",
    "item7 = \"Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-κB. A positive regulation between NOTCH and NF-κB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \"\n",
    "\n",
    "gt = \"Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGF-b1 signaling.\"\n",
    "\n",
    "set1 = {\"query\": 'What is the cause of the CADASIL?', \"evidences\": [item1, item2, item3]}\n",
    "set2 = {\"query\": 'What is the therapeutic target for CADASIL?', \"evidences\": [item3, item4, item5, item6, item7], \"ground_truth\": gt}\n",
    "\n",
    "# test_args = [(set2, set1),]\n",
    "\n",
    "contexts = [set2, ]\n",
    "queries = [set1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('examples_20230207.json') as f:\n",
    "    loaded_dic = json.load(f)\n",
    "    contexts = loaded_dic['contexts']\n",
    "    queries = loaded_dic['queries']\n",
    "    \n",
    "prompt_setup = Prompt(contexts, queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[5] \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[6] \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[7] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "summarize given evidences and query:TMEM16A is a therapeutic target for CADASIL. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[1]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[2]. Niclosamide can suppress enhance inflammation in CADASIL[3]. \n",
      "\n",
      "evidence 0: Role of NOTCH3 Mutations in the Cerebral Small Vessel Disease Cerebral Autosomal Dominant Arteriopathy ... (Stroke 2018). There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with ...[1] \n",
      "evidence 1: NOTCH3 signaling is essential for NF-kB activation in TLRactivated macrophages (Sci Rep 2020). Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage ...[5] \n",
      "evidence 2: Niclosamide repurposed for the treatment of inflammatory airway disease (JCI Insight 2019). Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal antiinflammatory compound that inhibits goblet ...[6] \n",
      "evidence 3: The Ca2+-gated channel TMEM16A amplifies capillary pericyte contraction and reduces ... (J Clin Invest. 2022). The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone[7] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "summarize given evidences and query:\n"
     ]
    }
   ],
   "source": [
    "num_shots = 1\n",
    "mode = 'base'\n",
    "prompt = prompt_setup.get_prompt(mode=mode, num_shots=num_shots)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[5] \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[6] \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[7] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "summarize given evidences and query:TMEM16A is a therapeutic target for CADASIL. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[1]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[2]. Niclosamide can suppress enhance inflammation in CADASIL[3]. \n",
      "\n",
      "evidence 0: Role of NOTCH3 Mutations in the Cerebral Small Vessel Disease Cerebral Autosomal Dominant Arteriopathy ... (Stroke 2018). There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with ...[1] \n",
      "evidence 1: NOTCH3 signaling is essential for NF-kB activation in TLRactivated macrophages (Sci Rep 2020). Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage ...[5] \n",
      "evidence 2: Niclosamide repurposed for the treatment of inflammatory airway disease (JCI Insight 2019). Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal antiinflammatory compound that inhibits goblet ...[6] \n",
      "evidence 3: The Ca2+-gated channel TMEM16A amplifies capillary pericyte contraction and reduces ... (J Clin Invest. 2022). The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone[7] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "summarize given evidences and query:\n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['TMEM16A is a therapeutic target for CADASIL.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_to_word = {}\n",
    "num_to_word[0]='zero-shot'\n",
    "num_to_word[1]='one-shot'\n",
    "num_to_word[2]='two-shot'\n",
    "num_to_word[3]='few-shot'\n",
    "\n",
    "output_file_name = f\"{model_name.replace('/', '-')}_{mode}-mode_{num_to_word[num_shots]}.txt\"\n",
    "\n",
    "with open(output_file_name, \"w\") as results: # mujeen_\n",
    "    results.write(f'<< Model: {model_name} >>\\n\\n')\n",
    "    # results.write()\n",
    "    print('< PROMPT >\\n')\n",
    "    print(prompt)\n",
    "    results.write('< PROMPT >\\n\\n')\n",
    "    results.write(prompt)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs.to(device)\n",
    "    outputs = model.generate(**inputs, max_length=1024)\n",
    "    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    print('\\n\\n< GENERATED >\\n')\n",
    "    print(generated)\n",
    "    print('\\n###############################################################################################################################################################\\n')\n",
    "    results.write('\\n\\n< GENERATED >\\n\\n')\n",
    "    results.write(*generated)\n",
    "    results.write('\\n\\n###############################################################################################################################################################\\n')\n",
    "        \n",
    "results.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce5cb1d3583c4bc4c26ba2d9d1424221025f98623b08f3ff314497f7b1d6b7f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
