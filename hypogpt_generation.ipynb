{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaeeun/.conda/envs/biomedlm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1, 2\"\n",
    "\n",
    "import torch\n",
    "print(torch.backends.cuda.is_built())\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDForConditionalGeneration(\n",
       "  (led): LEDModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): LEDEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(16384, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (query_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_global): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LEDDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "\n",
    "# model_name = 'google/flan-t5-small'\n",
    "# model_name = 'google/flan-t5-base'\n",
    "# model_name = 'google/flan-t5-small'\n",
    "# model_name = 'google/flan-t5-xl'\n",
    "# model_name = 'google/flan-t5-large'\n",
    "# model_name = 'google/flan-t5-xxl'\n",
    "# model_name = \"google/pegasus-pubmed\"\n",
    "# model_name = 'microsoft/BioGPT-Large'\n",
    "model_name = \"allenai/led-large-16384\"\n",
    "\n",
    "# model_name = \"stanford-crfm/BioMedLM\"\n",
    "\n",
    "\n",
    "# config.n_positions = 2048\n",
    "\n",
    "\n",
    "if model_name == \"microsoft/BioGPT-Large\":\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
    "elif model_name == \"stanford-crfm/BioMedLM\":\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "else:\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEDConfig {\n",
      "  \"_name_or_path\": \"allenai/led-large-16384\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "## experimenting for BioMedLM. Delete this cell later. \n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(config)\n",
    "\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models to run\n",
    "# bioGPT\n",
    "# bioGPT large\n",
    "# Flan-T5\n",
    "# google/pegasus-pubmed\n",
    "\n",
    "# ['base', 'FG_template', 'evidence-only']\n",
    "\n",
    "num_shots = 1\n",
    "mode = 'base'\n",
    "instruction = None\n",
    "# instruction = \"Answer the query: \"\n",
    "# instruction = \"Given the evidence, answer the query provided: \"\n",
    "# instruction = \"Based on the evidence provided, what is your conclusion regarding the query? \"\n",
    "# instruction = \"Write a summary of the evidence and query in your own words: \"\n",
    "# instruction = \"What are the potential implications of the given evidences in relation to the query? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syk is a therapeutic target for CADASIL[1]. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling[2]. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells[3]. Syk increases fibrosis through activation of TGF-b1 signaling[4].[5].\n"
     ]
    }
   ],
   "source": [
    "def tag_every_sentence(target: str):\n",
    "    # sentences = target.split('.')\n",
    "    tagged = [f'{sentence}[{i+1}].' for i, sentence in enumerate(target.split('.'))]\n",
    "    return ''.join(tagged)\n",
    "    \n",
    "target = 'Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGF-b1 signaling.'\n",
    "print(tag_every_sentence(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs propt template with tags to be replaced\n",
    "# could avoid the problem of overlapping with query by splitting contexts before passing it to the two functions \n",
    "import random\n",
    "# (contexts, queries, mode, num_shots, instruction=None):\n",
    "class Prompt:\n",
    "    def __init__(self, contexts, queries):\n",
    "        self.contexts = contexts\n",
    "        self.queries = queries\n",
    "        # self.mode = mode \n",
    "        # self.num_shots = num_shots\n",
    "            \n",
    "    def _get_context_template(self, contexts_subset, num_examples, ref_tag): # or you could return all possible templates as a list \n",
    "        \n",
    "        if num_examples==0:\n",
    "            return None\n",
    "            \n",
    "        # prefix on every example, suffix only at the very end of context template\n",
    "        example_part = \"\"\n",
    "        for idx_example in range(num_examples): # change this part for random sampling / all permutation / etc\n",
    "            example = contexts_subset[idx_example]\n",
    "            example_part += \"<EX_PREFIX>\"\n",
    "            for i, ev in enumerate(example['evidences']):\n",
    "                example_part += f\"evidence {i}: {ev}\"\n",
    "                if ref_tag: example_part +=f\"[{i+1}] \\n\"\n",
    "                else: example_part +=f\" \\n\"\n",
    "            \n",
    "            example_part += f\"query: {example['query']} \\n\"\n",
    "            example_part += \"<EX_OUTPUT>\" # either Output: or Instruction or both?\n",
    "            if ref_tag:\n",
    "                tagged_gt = tag_every_sentence(example['ground_truth'])\n",
    "                example_part += f\"{tagged_gt} \\n\"\n",
    "            else:\n",
    "            # tagged_gt = tag_every_sentence(example['ground_truth'])\n",
    "                example_part += f\"{example['ground_truth']} \\n\"\n",
    "        example_part += \"<EX_SUFFIX>\"\n",
    "            \n",
    "        return example_part\n",
    "    \n",
    "    \n",
    "\n",
    "    def _get_query_templates(self, queries_subset, ref_tag, no_evidence):\t# returns a list of all possible query templates\n",
    "        \n",
    "        templates = []\n",
    "        # dict_3 = {**dict_1,**dict_2}\n",
    "        \n",
    "        for query_idx in range(len(queries_subset)):\n",
    "            query = queries_subset[query_idx]\n",
    "            \n",
    "            if no_evidence:\n",
    "                query_part = \"\"\n",
    "            else:\n",
    "                query_part = \"<QR_PREFIX>\\n\" # probs will just get rid of it \n",
    "                for i, ev in enumerate(query['evidences']):\n",
    "                    query_part += f\"evidence {i}: {ev}\"\n",
    "                    if ref_tag: query_part +=f\"[{i+1}] \\n\"\n",
    "                    else: query_part +=f\" \\n\"\n",
    "               \n",
    "            query_part += f\"query: {query['query']}\"\n",
    "            query_part += \"<QR_SUFFIX>\" # probs instruction\n",
    "            \n",
    "            templates.append(query_part)\n",
    "            \n",
    "        return templates\n",
    "\n",
    "    def _transform_templates(self, context_template, query_templates, mode, instruction):\n",
    "\n",
    "        ## FG_template이면 query가 없고 evidence-only면 ... 뭐가 없지? 얘는 context를 날리는게 맞는 듯. \n",
    "        \n",
    "        assert (mode in ['base', 'FG_template', 'evidence-only']), \"invalid mode\"\n",
    "        \n",
    "        instruction = (\"Summarize given evidences and query: \" if instruction==None else instruction)\n",
    "        preface = \"I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs: \\n\"\n",
    "        \n",
    "        if mode==\"base\":\n",
    "            ex_prefix, ex_output, ex_suffix, qr_prefix, qr_suffix = \"\", instruction, \"\", \"\", instruction\n",
    "        elif mode==\"FG_template\": # query part irrelevant\n",
    "            ex_prefix, ex_output, ex_suffix = (preface + \"\\n\" + \"Input: \\n\"), \"Output: \\n\", \"\\nThe instruction was: \"\n",
    "        elif mode==\"evidence-only\": # context part irrelevant\n",
    "            context_template = None\n",
    "            qr_prefix, qr_suffix = \"\", \"\",\n",
    "            \n",
    "        # \n",
    "        context_transformed = context_template.replace('<EX_PREFIX>', ex_prefix).replace('<EX_OUTPUT>', ex_output).replace('<EX_SUFFIX>', ex_suffix) if (context_template!=None) else \"\"\n",
    "        \n",
    "        if mode=='FG_template':\n",
    "            queries_transformed = None\n",
    "        else:\n",
    "            queries_transformed = []\n",
    "            for query_template in query_templates:\n",
    "                temp = query_template.replace('<QR_PREFIX>', qr_prefix).replace('<QR_SUFFIX>', qr_suffix)\n",
    "                queries_transformed.append(temp)\n",
    "            \n",
    "        return context_transformed, queries_transformed\n",
    "        \n",
    "\n",
    "\n",
    "    def get_prompt(self, mode, num_shots, instruction=None, ref_tag=False, no_evidence=False):\n",
    "        \n",
    "\n",
    "        # randomly select from contexts num_shots contexts\n",
    "        indices = list(range(len(self.contexts)))  # create a list of indices from 0 to 9\n",
    "        context_indices = random.sample(indices, num_shots)\n",
    "        contexts = [self.contexts[i] for i in context_indices]\n",
    "        as_queries = [self.contexts[i] for i in range(len(self.contexts)) if i not in context_indices]\n",
    "        queries = self.queries + as_queries\n",
    "        \n",
    "        context_template = self._get_context_template(contexts, num_shots, ref_tag=ref_tag)\n",
    "        query_templates = self._get_query_templates(queries, ref_tag=ref_tag, no_evidence=no_evidence)\n",
    "        \n",
    "        context_transformed, queries_transformed = self._transform_templates(context_template, query_templates, mode, instruction)\n",
    "        \n",
    "        prompts = []\n",
    "        if mode != 'FG_template':\n",
    "            for query_transformed in queries_transformed:\n",
    "                prompt = context_transformed + query_transformed\n",
    "                prompts.append(prompt)\n",
    "        else:\n",
    "            prompts.append(context_transformed) # if mode is FG_template\n",
    "        \n",
    "        return prompts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for Google Sheets \n",
    "\n",
    "item1 = '''Here we report the characterization of the human Notch3 gene which we mapped to the CADASIL critical region. We have identified mutations in CADASIL patients that cause serious disruption of this gene, indicating that Notch3 could be the defective protein in CADASIL patients. All these missense mutations may result in severe disruption of the Notch3 protein, as suggested by the highly conserved nature of the aminoacid residues involved, particularly the cysteines that are key features of EGF likedomains24. These results indicate that these nucleotide substitutions are pathogenic mutations rather than rare polymorphisms.'''\n",
    "item2 = '''Linkage studies in other families enabled further refinement of this genetic interval16,17 and identification of the mutated gene as NOTCH3 (Notch homolog 3).18'''\n",
    "item3 = '''CADASIL, a hereditary vascular dementia, suggesting a role for Notch3 in vessel homeostasis (Joutel et al. 1996). CADASIL is a late-onset disorder, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of vSMC . maturation of vSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of vSMC.'''\n",
    "\n",
    "item3 = \"There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \"\n",
    "item4 = \"We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \"\n",
    "item5 = \"Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large\"\n",
    "item6 = \"Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK.\"\n",
    "item7 = \"Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-κB. A positive regulation between NOTCH and NF-κB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \"\n",
    "\n",
    "gt = \"Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGF-b1 signaling.\"\n",
    "\n",
    "set1 = {\"query\": 'What is the cause of the CADASIL?', \"evidences\": [item1, item2, item3]}\n",
    "set2 = {\"query\": 'What is the therapeutic target for CADASIL?', \"evidences\": [item3, item4, item5, item6, item7], \"ground_truth\": gt}\n",
    "\n",
    "# test_args = [(set2, set1),]\n",
    "\n",
    "contexts = [set2, ]\n",
    "queries = [set1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('examples_20230207.json') as f:\n",
    "    loaded_dic = json.load(f)\n",
    "    contexts = loaded_dic['contexts']\n",
    "    queries = loaded_dic['queries']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_setup = Prompt(contexts, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5]. \n",
      "\n",
      "evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis. Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship.[1] \n",
      "evidence 1: Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship. Neuropsychological consultation helped initiate removal of the MS diagnosis and treatment. Better understanding is needed among clinicians that MS and CADASIL are not known to co-exist, that no association has been found between MS and the NOTCH3 mutations that cause CADASIL, and that neuroimaging and clinical features can help distinguish between the two conditions in addition to genetic testing.[2] \n",
      "evidence 2: Antithrombotic drugs were used in three patients, in one for an unrelated coexisting prothrombotic condition. CADASIL does not seem to be associated with an unfavorable outcome of pregnancy either for women and fetuses. Patients and treating physicians should be reassured that pregnancy can be safely initiated in CADASIL, as there is no evidence to support a specific preventive antithrombotic treatment during pregnancy in CADASIL.[3] \n",
      "query: can nilvadipine help alleviate cadasil symptoms?Summarize given evidences and query: \n"
     ]
    }
   ],
   "source": [
    "# assert (mode in ['base', 'FG_template', 'evidence-only']), \"invalid mode\"\n",
    "\n",
    "# num_shots = 3\n",
    "# mode = 'FG_template'\n",
    "# instruction = ''\n",
    "\n",
    "prompts = prompt_setup.get_prompt(mode=mode, num_shots=num_shots, instruction=instruction, ref_tag=True)\n",
    "\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'allenai/led-base-16384': 16384}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.max_model_input_sizes)\n",
    "len_limit = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "## in progress. simplifying generation configuration process\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_length\": len_limit//2,\n",
    "    # \"min_length\": 1024,\n",
    "    'do_sample': True, \n",
    "    'top_k': 50,\n",
    "    'num_beams': 5,   \n",
    "    'early_stopping': True,\n",
    "}\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"max_length\": len_limit//2,\n",
    "    \"return_tensors\": \"pt\", # biomed by default only specifies this\n",
    "    \"truncation\": True,  \n",
    "}\n",
    "\n",
    "assert (generate_kwargs[\"max_length\"] <= len_limit and tokenizer_kwargs[\"max_length\"] <= len_limit), \"max_length exceeds max input length specified by the model.\"\n",
    "\n",
    "print(generate_kwargs[\"max_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd = /ssd1/chaeeun/biochatgpt_generation\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5]. \n",
      "\n",
      "evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis. Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship.[1] \n",
      "evidence 1: Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship. Neuropsychological consultation helped initiate removal of the MS diagnosis and treatment. Better understanding is needed among clinicians that MS and CADASIL are not known to co-exist, that no association has been found between MS and the NOTCH3 mutations that cause CADASIL, and that neuroimaging and clinical features can help distinguish between the two conditions in addition to genetic testing.[2] \n",
      "evidence 2: Antithrombotic drugs were used in three patients, in one for an unrelated coexisting prothrombotic condition. CADASIL does not seem to be associated with an unfavorable outcome of pregnancy either for women and fetuses. Patients and treating physicians should be reassured that pregnancy can be safely initiated in CADASIL, as there is no evidence to support a specific preventive antithrombotic treatment during pregnancy in CADASIL.[3] \n",
      "query: can nilvadipine help alleviate cadasil symptoms?Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['evidenceevidence evidence evidence evidence evidence 0: CADASIL.evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS.evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS). MS.evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS)....evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis.evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] ÃÂÃÂevidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] ÃÂÃÂevidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27)....This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] ÃÂÃÂevidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] ÃÂÃÂquery: What is the therapeutic target for CADASIL? ンジSummarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5]. ÃÂÃÂevidence 0: CADASIL is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis.evidence 0: CADASIL is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis.evidence 0: CADASIL is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis.evidence 0: CADASIL is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI)']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5]. \n",
      "\n",
      "evidence 0: Using both wild type mice and transgenic mice carrying the human mutant Notch3 gene (CADASIL mice), we have recently characterized the pathological features of CADASIL and determined the therapeutic efficacy of two hematopoietic growth factors, stem cell factor (SCF) and granulocyte colony-stimulating factor (G-CSF) in CADASIL. Our findings have revealed novel pathological changes in the endothelium of cerebral capillaries and in the neural stem cells (NSCs). We have also observed the impairment of cognitive function in CADASIL mice.[1] \n",
      "evidence 1: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available.[2] \n",
      "evidence 2: Today, CADASIL is known to every neurologist, but the disease has not yet revealed all its secrets. A lot of effort is still needed to understand the intimate mechanisms of the disease and the most efficient targets or approaches for the development of efficient therapeutics. The history of CADASIL will be further enriched by multiple ongoing research projects worldwide, at clinical and preclinical level, and will continue to enlighten research in the field of cerebral small vessel disorders.[3] \n",
      "query: what are the potential therapeutic targets for cadasil?Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['evidenceevidence 0: Using both wild type mice and transgenic mice carrying the human mutant Notch3 gene (CADASIL gene (CADASIL gene (CADASIL, R169C or C455R or C455R mutations were instead shown to lead to hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations generate hyperactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling.evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] ゼウスevidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] ゼウスevidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27)....This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] ゼウスevidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] 覚醒query: What is the therapeutic target for CADASIL? ModLoaderSummarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5].  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0evidence 0: Using both wild type mice and transgenic mice carrying the human mutant Notch3 gene (CADASIL mice), we have recently characterized the pathological features of CADASIL and determined the therapeutic efficacy of two hematopoietic growth factors, stem cell factor (SCF) and granulocyte colony-stimulating factor (G-CSF) in CADASIL. Our findings have revealed novel pathological changes in the endothelium of cerebral capillaries and in the neural stem cells (NSCs). We have also observed the impairment of cognitive function in CADASIL mice.[1] ゼウスevidence 1: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. A lot of effort is still needed to understand the intimate mechanisms of the disease and the most efficient targets or approaches for the development of efficient therapeutics. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available.[2] >>\\\\evidence 2: Today, CADASIL is known to every neurologist, but the disease has not yet revealed all its secrets.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5]. \n",
      "\n",
      "evidence 0: Syk inhibition was found to stabilize microtubules and potentiate paclitaxel activity in cellular models of taxane-resistant ovarian cancers. We further studied the effects of Syk inhibition on paclitaxel activity in Syk(+) ovarian cancer cell models and in variants selected for taxane resistance. Syk inhibition was accomplished using RNAi and by exposure to the small molecule competitive inhibitor R406, the active metabolite of fostamatinib.[1] \n",
      "evidence 1: Therefore, inhibition of the inflammatory macrophages would be a promising approach to attenuate NASH. In this study, we studied the implication of SYK pathway in NASH, and investigated PLGA nanoparticles-based delivery of SYK pathway inhibitor as an effective and promising therapeutic approach for the treatment of NASH. We found positive correlation between SYK expression with the pathogenesis of NASH and alcoholic hepatitis in patients.[2] \n",
      "evidence 2: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available.[3] \n",
      "query: would SYK inhibition be potential treatment for cadasil?Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['evidenceevidence 0: evidence 0: evidence 0: Syk inhibition was found to stabilize microtubules and potentiate microtubules and potentiate paclitaxel activity in cellular models of SYK pathway in NASH.evidence 0: Syk inhibition was found to stabilize microtubules and potentiate paclitaxel activity in cellular models of the inflammatory macrophages would be a promising approach to attenuate NASH. We further studied the effects of Syk inhibition on paclitaxel activity in Syk(+) ovarian cancer cell models and in variants selected for taxane resistance. In this study, we studied the implication of SYK pathway in NASH, and investigated PLGA nanoparticles-based delivery of SYK pathway inhibitor as an effective and promising therapeutic approach for the treatment of NASH.evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] 覚醒evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] luajevidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27)....This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] ForgeModLoaderevidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] 覚醒query: What is the therapeutic target for CADASIL? ModLoaderSummarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5]. ..............evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. We found positive correlation between SYK expression with the pathogenesis of NASH and alcoholic hepatitis in patients.[2] ````evidence 2: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. ..............evidence 0: Syk inhibition was found to stabilize microtubules and potentiate paclitaxel activity in cellular models of taxane-resistant ovarian cancers. Syk inhibition was accomplished using RNAi and by exposure to the small molecule competitive inhibitor R406, the active metabolite of fostamatinib.[1] =-=-=-=-=-=-=-=-evidence 1: Therefore, inhibition of the inflammatory macrophages would be a promising approach to attenuate NASH. ..............evidence 0: Syk inhibition was found to stabilize microtubules and potentiate paclitaxel activity in cellular models of taxane-resistant ovarian cancers. Syk inhibition was accomplished using RNAi and by exposure to the small molecule']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36[1] \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results,[2] \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31).[3] \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy[4] \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: TMEM16A is a therapeutic target for CADASIL[1]. Ischemia in CADASIL activates TMEM16A, Ca2+ gated Cl- channel, which regulates pericyte tone and induces capillary constriction[2]. Potent inhibitors of TMEM16A, niclosamide has antiinflammtory effect[3]. Niclosamide can suppress enhance inflammation in CADASIL[4].[5]. \n",
      "\n",
      "evidence 0: Here we report the characterization of the human Notch3 gene which we mapped to the CADASIL critical region. We have identified mutations in CADASIL patients that cause serious disruption of this gene, indicating that Notch3 could be the defective protein in CADASIL patients.All these missense mutations may result in severe disruption of the Notch3 protein, as suggested by the highly conserved nature of the aminoacid residues involved, particularly the cysteines that are key features of EGF-like domains. These results indicate that these nucleotide substitutions are pathogenic mutations rather than rare polymorphisms.[1] \n",
      "evidence 1: Linkage studies in other families enabled further refinement of this genetic interval16,17 and identification of the mutated gene as NOTCH3 (Notch homolog 3).18[2] \n",
      "evidence 2: CADASIL, a hereditary vascular dementia, suggesting a role for Notch3 in vessel homeostasis (Joutel et al. 1996). CADASIL is a late-onset disorder, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of VSMC. ...maturation of VSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of VSMC.[3] \n",
      "query: What is the cause of the CADASIL?Summarize given evidences and query: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[39m# outputs = model.generate(**inputs, max_length=2048, min_length=100, num_beams=5)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m     34\u001b[0m generated \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[39m# print(generated)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# generated = *generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/transformers/generation_utils.py:1621\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1613\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1614\u001b[0m         input_ids,\n\u001b[1;32m   1615\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_beams \u001b[39m*\u001b[39m num_return_sequences,\n\u001b[1;32m   1616\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1617\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1618\u001b[0m     )\n\u001b[1;32m   1620\u001b[0m     \u001b[39m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_sample(\n\u001b[1;32m   1622\u001b[0m         input_ids,\n\u001b[1;32m   1623\u001b[0m         beam_scorer,\n\u001b[1;32m   1624\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1625\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1626\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1627\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1628\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1629\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1630\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1631\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1632\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1635\u001b[0m \u001b[39melif\u001b[39;00m is_group_beam_gen_mode:\n\u001b[1;32m   1636\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/transformers/generation_utils.py:3059\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3055\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3057\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3060\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3061\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3062\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3063\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3064\u001b[0m )\n\u001b[1;32m   3066\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3067\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/transformers/models/led/modeling_led.py:2436\u001b[0m, in \u001b[0;36mLEDForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2432\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   2433\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   2434\u001b[0m         )\n\u001b[0;32m-> 2436\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mled(\n\u001b[1;32m   2437\u001b[0m     input_ids,\n\u001b[1;32m   2438\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   2439\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   2440\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   2441\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   2442\u001b[0m     global_attention_mask\u001b[39m=\u001b[39;49mglobal_attention_mask,\n\u001b[1;32m   2443\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   2444\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   2445\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   2446\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   2447\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   2448\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   2449\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   2450\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2451\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2452\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   2453\u001b[0m )\n\u001b[1;32m   2454\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   2456\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/transformers/models/led/modeling_led.py:2302\u001b[0m, in \u001b[0;36mLEDModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2294\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m LEDEncoderBaseModelOutput(\n\u001b[1;32m   2295\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   2296\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2297\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2298\u001b[0m         global_attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2299\u001b[0m     )\n\u001b[1;32m   2301\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2302\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   2303\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   2304\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   2305\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   2306\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   2307\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   2308\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   2309\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   2310\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   2311\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   2312\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2313\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2314\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   2315\u001b[0m )\n\u001b[1;32m   2317\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   2318\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/transformers/models/led/modeling_led.py:2166\u001b[0m, in \u001b[0;36mLEDDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   2156\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   2157\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2164\u001b[0m     )\n\u001b[1;32m   2165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2166\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   2167\u001b[0m         hidden_states,\n\u001b[1;32m   2168\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcombined_attention_mask,\n\u001b[1;32m   2169\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   2170\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   2171\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   2172\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   2173\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   2174\u001b[0m         ),\n\u001b[1;32m   2175\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   2176\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2177\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   2178\u001b[0m     )\n\u001b[1;32m   2180\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2182\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/transformers/models/led/modeling_led.py:1087\u001b[0m, in \u001b[0;36mLEDDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m cross_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1087\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_attn(\n\u001b[1;32m   1088\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1089\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1090\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1091\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1092\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m   1093\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1094\u001b[0m )\n\u001b[1;32m   1095\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m   1096\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/biomedlm/lib/python3.9/site-packages/transformers/models/led/modeling_led.py:850\u001b[0m, in \u001b[0;36mLEDDecoderAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    847\u001b[0m bsz, tgt_len, embed_dim \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[1;32m    849\u001b[0m \u001b[39m# get query proj\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(hidden_states) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaling\n\u001b[1;32m    851\u001b[0m \u001b[39m# get key, value proj\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[39mif\u001b[39;00m is_cross_attention \u001b[39mand\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    853\u001b[0m     \u001b[39m# reuse k,v, cross_attentions\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "num_to_word = {}\n",
    "num_to_word[0]='zero-shot'\n",
    "num_to_word[1]='one-shot'\n",
    "num_to_word[2]='two-shot'\n",
    "num_to_word[3]='few-shot'\n",
    "\n",
    "output_file_name = f\"{model_name.replace('/', '-')}_{mode}-mode_{num_to_word[num_shots]}\"\n",
    "output_dic = {\n",
    "    'output' : [],\n",
    "    'config' : {'mode': mode, 'num_shots': num_shots}\n",
    "}\n",
    "\n",
    "\n",
    "# cwd = os.path.dirname(os.path.abspath(__file__))\n",
    "cwd = os.getcwd()\n",
    "print(f'cwd = {cwd}')\n",
    "file_name_json = os.path.join(cwd, f\"{output_file_name}.json\")\n",
    "\n",
    "with open(f'{output_file_name}.txt', \"w\") as results: # mujeen_\n",
    "    results.write(f'<< Model: {model_name} >>\\n\\n')\n",
    "\n",
    "    for prompt in prompts:\n",
    "\n",
    "        print('< PROMPT >\\n')\n",
    "        print(prompt)\n",
    "        results.write('< PROMPT >\\n\\n')\n",
    "        results.write(prompt)\n",
    "        \n",
    "        # inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        inputs = tokenizer(prompt, **tokenizer_kwargs)\n",
    "        inputs.to(device)\n",
    "        # outputs = model.generate(**inputs, max_length=2048, min_length=100, num_beams=5)\n",
    "        outputs = model.generate(**inputs, **generate_kwargs)\n",
    "        generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # print(generated)\n",
    "        # generated = *generated\n",
    "        \n",
    "        output_dic['output'].append({'prompt': prompt, 'generated': generated})\n",
    "\n",
    "        print('\\n\\n< GENERATED >\\n')\n",
    "        print(generated)\n",
    "        print('\\n###############################################################################################################################################################\\n')\n",
    "        results.write('\\n\\n< GENERATED >\\n\\n')\n",
    "        results.write(*generated)\n",
    "        results.write('\\n\\n###############################################################################################################################################################\\n')\n",
    "    \n",
    "\n",
    "    with open(file_name_json, \"w\") as outfile: \n",
    "        json.dump(output_dic, outfile)\n",
    "    outfile.close()\n",
    "results.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: CADASIL) is sometimes misdiagnosed as multiple sclerosis (MS). MS and CADASIL are not known to co-occur and brain magnetic resonance imagining (MRI) findings can help with differential diagnosis. Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship. \n",
      "evidence 1: Despite the availability of this information, a case report is presented of a 61-year-old woman who was misdiagnosed with MS at age 50, tested positive for CADASIL at age 56, described incorrectly as having both conditions simultaneously, and continued on MS disease-modifying medications, resulting in financial and physical hardship. Neuropsychological consultation helped initiate removal of the MS diagnosis and treatment. Better understanding is needed among clinicians that MS and CADASIL are not known to co-exist, that no association has been found between MS and the NOTCH3 mutations that cause CADASIL, and that neuroimaging and clinical features can help distinguish between the two conditions in addition to genetic testing. \n",
      "evidence 2: Antithrombotic drugs were used in three patients, in one for an unrelated coexisting prothrombotic condition. CADASIL does not seem to be associated with an unfavorable outcome of pregnancy either for women and fetuses. Patients and treating physicians should be reassured that pregnancy can be safely initiated in CADASIL, as there is no evidence to support a specific preventive antithrombotic treatment during pregnancy in CADASIL. \n",
      "query: can nilvadipine help alleviate cadasil symptoms? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL. Nilvadipine is a non-steroidal anti-inflammatory drug (NSAID) that is used to treat CADASIL.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Using both wild type mice and transgenic mice carrying the human mutant Notch3 gene (CADASIL mice), we have recently characterized the pathological features of CADASIL and determined the therapeutic efficacy of two hematopoietic growth factors, stem cell factor (SCF) and granulocyte colony-stimulating factor (G-CSF) in CADASIL. Our findings have revealed novel pathological changes in the endothelium of cerebral capillaries and in the neural stem cells (NSCs). We have also observed the impairment of cognitive function in CADASIL mice. \n",
      "evidence 1: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available. \n",
      "evidence 2: Today, CADASIL is known to every neurologist, but the disease has not yet revealed all its secrets. A lot of effort is still needed to understand the intimate mechanisms of the disease and the most efficient targets or approaches for the development of efficient therapeutics. The history of CADASIL will be further enriched by multiple ongoing research projects worldwide, at clinical and preclinical level, and will continue to enlighten research in the field of cerebral small vessel disorders. \n",
      "query: what are the potential therapeutic targets for cadasil? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Syk inhibition was found to stabilize microtubules and potentiate paclitaxel activity in cellular models of taxane-resistant ovarian cancers. We further studied the effects of Syk inhibition on paclitaxel activity in Syk(+) ovarian cancer cell models and in variants selected for taxane resistance. Syk inhibition was accomplished using RNAi and by exposure to the small molecule competitive inhibitor R406, the active metabolite of fostamatinib. \n",
      "evidence 1: Therefore, inhibition of the inflammatory macrophages would be a promising approach to attenuate NASH. In this study, we studied the implication of SYK pathway in NASH, and investigated PLGA nanoparticles-based delivery of SYK pathway inhibitor as an effective and promising therapeutic approach for the treatment of NASH. We found positive correlation between SYK expression with the pathogenesis of NASH and alcoholic hepatitis in patients. \n",
      "evidence 2: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy (CADASIL) is the most common condition of hereditary stroke and vascular dementia. CADASIL is caused by Notch3 mutation, leading to progressive degeneration of vascular smooth muscle cells (VSMCs) of the small arteries in the brain. However, the pathogenesis of CADASIL remains largely unknown, and treatment that can stop or delay the progression of CADASIL is not yet available. \n",
      "query: would SYK inhibition be potential treatment for cadasil? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMCs) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. Inhibition of the inflammatory macrophages would be a promising approach to attenuate NASH. In this study, we studied the implication of SYK pathway in NASH, and investigated PLGA nanoparticles-based delivery of SYK pathway inhibitor as an effective and promising therapeutic approach for the treatment of NASH. We found positive correlation between SYK expression with the pathogenesis of NASH and alcoholic hepatitis in patients.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Here we report the characterization of the human Notch3 gene which we mapped to the CADASIL critical region. We have identified mutations in CADASIL patients that cause serious disruption of this gene, indicating that Notch3 could be the defective protein in CADASIL patients.All these missense mutations may result in severe disruption of the Notch3 protein, as suggested by the highly conserved nature of the aminoacid residues involved, particularly the cysteines that are key features of EGF-like domains. These results indicate that these nucleotide substitutions are pathogenic mutations rather than rare polymorphisms. \n",
      "evidence 1: Linkage studies in other families enabled further refinement of this genetic interval16,17 and identification of the mutated gene as NOTCH3 (Notch homolog 3).18 \n",
      "evidence 2: CADASIL, a hereditary vascular dementia, suggesting a role for Notch3 in vessel homeostasis (Joutel et al. 1996). CADASIL is a late-onset disorder, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of VSMC. ...maturation of VSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of VSMC. \n",
      "query: What is the cause of the CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['CADASIL is a hereditary vascular dementia, and neurological symptoms arise from a slowly developing systemic vasculopathy, characterized ultimately by degeneration of VSMC....maturation of VSMC, and ends around P28 when the artery acquires its final shape. We identify Notch3 to be the first key player of this process, by regulating cell-autonomously the arterial differentiation and maturation of VSMC.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "evidence 2: Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal anti-inflammatory compound that inhibits goblet cell degranulation and suppresses asthma phenotype (4, 27). ...This result suggests an antiinflammatory effect of niclosamide, which corresponds well to inhibition of allergic lung inflammation by the TMEM16A inhibitor benzbromarone (3, 31). \n",
      "evidence 3: The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone; (b) TMEM16A is activated during ischemia and evokes a long-lasting pericytemediated capillary constriction that reduces CBF and favors neutrophil and platelet stalling; (c) genetic analysis suggests that increased TMEM16A expression is associated with poor recovery after ischemic stroke (and the genetic proxy \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Question: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Role of NOTCH3 Mutations in the Cerebral Small Vessel Disease Cerebral Autosomal Dominant Arteriopathy ... (Stroke 2018). There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with... \n",
      "evidence 1: Differences in proliferation rate between CADASIL and control vascular smooth muscle cells are ... (J Cell Mol Med 2018). We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. ...showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic... \n",
      "evidence 2: The Role of Vascular Smooth Muscle Cells in Arterial Remodeling: Focus on Calcification ... (Int J Mol Sci 2019). Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused... \n",
      "evidence 3: Spleen Tyrosine Kinase (SYK) in the Progression of Peritoneal Fibrosis Through Activation of the ... (Med Sci Monit 2019). Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase... \n",
      "evidence 4: NOTCH3 signaling is essential for NF-kB activation in TLRactivated macrophages (Sci Rep 2020). Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage... \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of the TGFb1 signaling pathway.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n",
      "< PROMPT >\n",
      "\n",
      "evidence 0: There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with regard to downstream signaling or whether CADASIL mutations are neutral in terms of Notch signaling. Whereas the R169C mutation appeared to lead to hyperactive Notch signaling (see above), the R1031C or C455R mutations were instead shown to be hypoactive,36 \n",
      "evidence 1: We believe that increased TGFb3 reflects an inflammatory condition and even an involvement of TGFb in fibrosis in CADASIL. Support for our hypothesis is the outcome of several studies that showed NOTCH3 and TGFb1 signalling play a key role in the pathogenesis and progression of chronic cardiovascular disease. \n",
      "evidence 2: Over time, VSMCs apoptosis leads to fibrosis and thickening of the arterial wall, progressive lumen stenosis and vascular insufficiency that makes the already poorly perfused terminal regions particularly susceptible to infarcts. SMCs degeneration is followed by the emergence of large,... \n",
      "evidence 3: Previously published studies have shown that TGF-b signaling is closely associated with the activity of SYK, and the kinase activity of SYK is essential for the activation of some signaling receptor downstream effector molecules[25]. ...bidirectional[33]. In the present study, the activation of SYK was shown to increase the progression of peritoneal fibrosis through activation of the TGF-b1/Smad3 signaling pathway, and inhibition of TGF-b1 also resulted in down-regulation of SYK. \n",
      "evidence 4: Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage activation and identified its prominent and specific role in the activation of NF-kB. A positive regulation between NOTCH and NF-kB signaling pathway has been described previously in macrophages isolated from patients with atherosclerosis. In those patients, and in contrast with our results, \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of TGFb1 signaling. \n",
      "\n",
      "evidence 0: Role of NOTCH3 Mutations in the Cerebral Small Vessel Disease Cerebral Autosomal Dominant Arteriopathy ... (Stroke 2018). There is currently no consensus on whether CADASIL mutations generate hyperactive or hypoactive Notch3 proteins with... \n",
      "evidence 1: NOTCH3 signaling is essential for NF-kB activation in TLRactivated macrophages (Sci Rep 2020). Our work confirms and highlights the relevance of NOTCH3 expression and signaling in pro-inflammatory macrophage... \n",
      "evidence 2: Niclosamide repurposed for the treatment of inflammatory airway disease (JCI Insight 2019). Niclosamide and derivatives - potent inhibitors of TMEM16A/F and Ca2+ signaling. NFA is a nonsteroidal antiinflammatory compound that inhibits goblet... \n",
      "evidence 3: The Ca2+-gated channel TMEM16A amplifies capillary pericyte contraction and reduces ... (J Clin Invest. 2022). The key findings of this study are that (a) Cl- fluxes mediated by the Ca2+-gated channel TMEM16A are a crucial determinant of pericyte tone \n",
      "query: What is the therapeutic target for CADASIL? \n",
      "Summarize given evidences and query: \n",
      "\n",
      "\n",
      "< GENERATED >\n",
      "\n",
      "['Syk is a therapeutic target for CADASIL. In CADASIL, proinflammatory signaling is activated through the increase of TGFb1 signaling. The apoptosis of vascular smooth muscle cell (VSMC) and increased TGFb1 signaling cause fibrosis and vascular abnormalities in vascular epithelial cells. Syk increases fibrosis through activation of the TGFb1 signaling pathway.']\n",
      "\n",
      "###############################################################################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{output_file_name}.txt', \"w\") as results: # mujeen_\n",
    "    results.write(f'<< Model: {model_name} >>\\n\\n')\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print('< PROMPT >\\n')\n",
    "        print(prompt)\n",
    "        results.write('< PROMPT >\\n\\n')\n",
    "        results.write(prompt)\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        inputs.to(device)\n",
    "        outputs = model.generate(**inputs, max_length=1024, min_length=100, num_beams=5)\n",
    "        generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # output_dic['output'].append({'prompt': prompt, 'generated': *generated})\n",
    "\n",
    "        print('\\n\\n< GENERATED >\\n')\n",
    "        print(generated)\n",
    "        print('\\n###############################################################################################################################################################\\n')\n",
    "        results.write('\\n\\n< GENERATED >\\n\\n')\n",
    "        results.write(*generated)\n",
    "        results.write('\\n\\n###############################################################################################################################################################\\n')\n",
    "        \n",
    "results.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedlm",
   "language": "python",
   "name": "biomedlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce5cb1d3583c4bc4c26ba2d9d1424221025f98623b08f3ff314497f7b1d6b7f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
